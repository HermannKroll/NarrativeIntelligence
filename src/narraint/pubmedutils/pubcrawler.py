import time
import urllib.request
import xml.etree.ElementTree as ET
from datetime import datetime
from urllib.parse import quote




# Query pmid's from pmed
# db: pubmed / pmc
from narrant.progress import print_progress_with_eta
from narrant.util.helpers import chunks


def pubmed_crawl_pmids(query, mail='ex@sample.com', tool='sampletool', db='pubmed', retmax=20000):
    """
    queries for PMIDs on pubmed by using a given query
    :param query: given query term
    :param mail: mail address to identify who is querying
    :param tool: toolname to identify who is querying
    :param db: which database should be queried? default (pubmed) also possible: (pmc)
    :param retmax: max of ids to query fore
    :return: a list of ids
    """
    # wait amount specifc amount of time
    time.sleep(1)
    # url callls
    domain = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?email={}&tool={}&db={}&term='.format(mail,
                                                                                                              tool, db)
    param = "&retmax={}".format(retmax)

    # get ids for sentence (empty list)
    pmids = []
    try:
        # encode sentence as url
        query_enc = quote(query, safe='')
        # construct final url
        url = domain + query_enc + param
        print("Crawling {}".format(url))
        # http get
        contents = urllib.request.urlopen(url).read()
        # convert result to xml
        xml_root = ET.fromstring(contents)

        # extract idlist of result
        for pmid_element in xml_root.find('IdList'):
            pmid = pmid_element.text
            # add all id's to sentence's pmids
            pmids.append(pmid)

        return pmids
    except:
        print("Error by crawling pmids for query: {}".format(query))
        return None


def store_pmids_to_file(pmids, filename, add_pmc_prefix=False):
    """
    stores a list of integers (PMIDs) to a file
    :param pmids: list of ids
    :param filename: file to write ids to
    :param add_pmc_prefix: should add a 'pmc' prefix?
    :return: nothing
    """
    print('Saving {} pmids to file: {}'.format(len(pmids), filename))
    i = 0
    with open(filename, 'w') as f:
        for pmid in pmids[0:-1]:
            i += 1
            if add_pmc_prefix:
                f.write('PMC{}\n'.format(pmid))
            else:
                f.write('{}\n'.format(pmid))
        # last line without \nâ€š
        i += 1
        if add_pmc_prefix:
            f.write('PMC{}'.format(pmids[-1]))
        else:
            f.write('PMC{}'.format(pmids[-1]))
    print('IDs saved! (Amount: {})'.format(i))


# Crawl pubtator documents
# following formats are allowed "PubTator"; //"JSON"; // "BioC";
def pubtator_crawl_pubtator_documents(pmids, format):
    """
    crawls pubtator tagged documents for a list of PMIDs
    :param pmids: list of ids
    :param format: PubTator, JSON, BioC
    :return: a string with the context of the resulting document
    """
    # wait amount specifc amount of time
    time.sleep(1)

    if format not in ['JSON', 'BioC', 'PubTator']:
        print('Format not supported: {}'.format(format))
        print('Supported formats: JSON, BioC, PubTator')
        return None
    if len(pmids) > 100 and format is not 'PubTator':
        print('Only in PubTator format more than 100 pmids are supported...')
        return None

    domain = 'https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/RESTful/tmTool.cgi/'
    concept_mode = 'BioConcept'
    start_time = datetime.now()
    if len(pmids) <= 100:
        # compute pmid str
        pmid_str = ''
        for pmid in pmids:
            pmid_str += pmid + ','
        pmid_str = pmid_str[:-1]

        url = '{}/{}/{}/{}'.format(domain, concept_mode, pmid_str, format)
        # print("Crawling {}".format(url))
        # http get
        contents = urllib.request.urlopen(url).read().decode('utf-8')
        return contents
    else:
        result_str = ''
        # we assume format mode is PubTator (see check above)
        i = 1
        chunk_size = int(len(pmids) / 100) + 1
        for c in chunks(pmids, 100):
            # compute pmid str
            pmid_str = ''
            for pmid in c:
                pmid_str += pmid + ','
            pmid_str = pmid_str[:-1]
            # url call
            url = '{}/{}/{}/{}'.format(domain, concept_mode, pmid_str, format)
            print_progress_with_eta('Crawling PMIDs from PubTator...', i, chunk_size, start_time)
            # http get
            result_str += urllib.request.urlopen(url).read().decode('utf-8')
            # wait amount specifc amount of time
            time.sleep(5)
            i += 1
        return result_str


def pubtator_crawl_pubtator_documents_with_query(query, mail='ex@sample.com', tool='sampletool'):
    """
    connects a pubmed query and automatically queries for all documents on pubtator
    :param query: query on pubmed
    :param mail: mail address to identify who is querying
    :param tool: toolname to identify who is querying
    :return: the resulting document from pubtator
    """
    print("Crawling PMIDs for query: {}".format(query))
    pmids = pubmed_crawl_pmids(query, mail, tool)
    print("{} PMIDs queried".format(len(pmids)))
    print("Crawling PubTator documents...")
    result = pubtator_crawl_pubtator_documents(pmids, 'PubTator')
    print("Crawling finished!")
    return result
